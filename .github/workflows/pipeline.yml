name: Data Pipeline

on:
  schedule:
    - cron: '0 0 * * *'  # Run daily at midnight
  workflow_dispatch:

env:
  GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  BQ_DATASET_RAW: staging
  BQ_DATASET_WAREHOUSE: warehouse
  GCS_BUCKET_NAME: ${{ secrets.GCP_PROJECT_ID }}-raw-data

jobs:
  ingestion:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      - name: Download Data (Kaggle)
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: python src/ingestion/kaggle_downloader.py
      - name: Upload to GCS
        run: python src/ingestion/gcs_uploader.py
      - name: Load to BigQuery
        run: python src/ingestion/bigquery_loader.py

  transformation:
    needs: ingestion
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install dbt
        run: pip install dbt-bigquery
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      - name: Run dbt
        run: |
          cd dbt
          dbt deps
          dbt run --profiles-dir .
          dbt test --profiles-dir .
        env:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ steps.auth.outputs.credentials_file_path }}
