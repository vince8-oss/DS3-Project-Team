{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad36d95",
   "metadata": {},
   "source": [
    "Step 1: Extract from Kaggle\n",
    "- Setup environment variable\n",
    "- Store raw CSVs in a structured folder (data/raw/olist_orders.csv, etc.) so Meltano can ingest cleanly.\n",
    "\n",
    "Step 2: Load into BigQuery with Meltano\n",
    "- Configure Meltano extractor (tap-kaggle or tap-csv) to read raw CSVs.\n",
    "- Configure loader (target-bigquery) with your GCP project credentials.\n",
    "- Define schema mapping in Meltano so each CSV → BigQuery table:\n",
    "- customers.csv → raw_customers\n",
    "- orders.csv → raw_orders\n",
    "- order_items.csv → raw_order_items\n",
    "- etc.\n",
    "This ensures raw layer consistency before DBT transformations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca1279",
   "metadata": {},
   "source": [
    "# Olist Brazilian E‑Commerce\n",
    "\n",
    "This notebook implements the plan you requested:\n",
    "\n",
    "- **Step 1: Extract from Kaggle** — set environment variables and download raw CSVs into `data/raw/`.\n",
    "- **Step 2: Load into BigQuery with Meltano** — configure a CSV extractor and `target-bigquery` loader, map CSVs → raw tables.\n",
    "- **Step 3: Transform with dbt** — create staging models, facts, dimensions, and tests.\n",
    "\n",
    "Edit the environment variables and file paths in the first code cell before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d530db7",
   "metadata": {},
   "source": [
    "### Notes before running\n",
    "\n",
    "- This notebook assumes you have Python, `pip`, and access to the internet from the environment where you run it.\n",
    "- You must **not** commit secrets. Use environment variables for credentials.\n",
    "- The notebook writes configuration files (Meltano, dbt `profiles.yml`, and sample SQL models) into the working directory so you can iterate locally or in a CI/CD pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173acf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAGGLE_USERNAME: kieronsiriban\n",
      "GCP_PROJECT: algebraic-road-478012-u9\n",
      "RAW_DIR: /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Edit these variables before running\n",
    "import os\n",
    "\n",
    "# Kaggle credentials (set as env vars; do NOT hardcode in shared repos)\n",
    "os.environ['KAGGLE_USERNAME'] = os.getenv('KAGGLE_USERNAME', 'kiessxxxxxx')  # replace with your Kaggle username\n",
    "os.environ['KAGGLE_KEY'] = os.getenv('KAGGLE_KEY', 'AAAA_AAA9999999999999') # replace with your Kaggle API key\n",
    "\n",
    "# GCP project and service account JSON path\n",
    "os.environ['GCP_PROJECT'] = os.getenv('GCP_PROJECT', 'aaaaaaaa') # replace with your GCP project ID\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.getenv('GOOGLE_APPLICATION_CREDENTIALS', '/home/dsai/5m-data-2.6-data-pipelines-orchestration/aaaaaaaaa.json')\n",
    "\n",
    "# Local paths used by this notebook\n",
    "BASE_DIR = os.path.abspath('.')\n",
    "RAW_DIR = os.path.join(BASE_DIR, 'data', 'raw')\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "print('KAGGLE_USERNAME:', os.environ['KAGGLE_USERNAME'])\n",
    "print('GCP_PROJECT:', os.environ['GCP_PROJECT'])\n",
    "print('RAW_DIR:', RAW_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde938e9",
   "metadata": {},
   "source": [
    "## Step 1: Extract from Kaggle\n",
    "\n",
    "This cell installs the Kaggle API client, downloads the Olist dataset, and places CSVs into `data/raw/` with predictable filenames so Meltano can ingest them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5286d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Dataset URL: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
      "Download complete. Listing CSVs:\n",
      "- olist_order_payments_dataset.csv\n",
      "- olist_orders_dataset.csv\n",
      "- olist_products_dataset.csv\n",
      "- olist_order_reviews_dataset.csv\n",
      "- olist_customers_dataset.csv\n",
      "- olist_sellers_dataset.csv\n",
      "- olist_geolocation_dataset.csv\n",
      "- olist_order_items_dataset.csv\n",
      "- product_category_name_translation.csv\n",
      "\n",
      "Copied CSVs to /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>payment_sequential</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>payment_installments</th>\n",
       "      <th>payment_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b81ef226f3fe1789b1e8b2acac839d17</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>8</td>\n",
       "      <td>99.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a9810da82917af2d9aefd1278f1dcfa0</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>1</td>\n",
       "      <td>24.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25e8ea4e93396b6fa0d3dd708e76c1bd</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>1</td>\n",
       "      <td>65.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ba78997921bbcdc1373bb41e913ab953</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>8</td>\n",
       "      <td>107.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42fdf880ba16b47b59251dd489d4441a</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>2</td>\n",
       "      <td>128.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id  payment_sequential payment_type  \\\n",
       "0  b81ef226f3fe1789b1e8b2acac839d17                   1  credit_card   \n",
       "1  a9810da82917af2d9aefd1278f1dcfa0                   1  credit_card   \n",
       "2  25e8ea4e93396b6fa0d3dd708e76c1bd                   1  credit_card   \n",
       "3  ba78997921bbcdc1373bb41e913ab953                   1  credit_card   \n",
       "4  42fdf880ba16b47b59251dd489d4441a                   1  credit_card   \n",
       "\n",
       "   payment_installments  payment_value  \n",
       "0                     8          99.33  \n",
       "1                     1          24.39  \n",
       "2                     1          65.71  \n",
       "3                     8         107.78  \n",
       "4                     2         128.45  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install Kaggle client if needed\n",
    "!pip install --quiet kaggle pandas\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import zipfile\n",
    "import shutil\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "dataset = 'olistbr/brazilian-ecommerce'\n",
    "download_path = '/tmp/olist_kaggle_download'\n",
    "import os\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "print('Downloading dataset...')\n",
    "api.dataset_download_files(dataset, path=download_path, unzip=True)\n",
    "print('Download complete. Listing CSVs:')\n",
    "csvs = glob.glob(os.path.join(download_path, '*.csv'))\n",
    "for f in csvs:\n",
    "    print('-', os.path.basename(f))\n",
    "\n",
    "# Copy CSVs to data/raw with stable names\n",
    "for f in csvs:\n",
    "    dest = os.path.join(RAW_DIR, os.path.basename(f))\n",
    "    shutil.copyfile(f, dest)\n",
    "print('\\nCopied CSVs to', RAW_DIR)\n",
    "\n",
    "# Quick peek at one file\n",
    "pd.read_csv(os.path.join(RAW_DIR, os.path.basename(csvs[0]))).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run in terminal \n",
    "\n",
    "# ensure meltano CLI is available\n",
    "pip install --upgrade meltano\n",
    "\n",
    "# initialize project if you haven't\n",
    "meltano init olist_pipeline\n",
    "cd olist_pipeline   \n",
    "\n",
    "# add plugins (this updates meltano.yml and installs the correct packages)\n",
    "meltano add extractor tap-csv\n",
    "meltano add loader target-bigquery\n",
    "\n",
    "# verify plugins\n",
    "meltano plugins list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e935111",
   "metadata": {},
   "source": [
    "## Step 2: Load into BigQuery with Meltano\n",
    "\n",
    "We will scaffold a Meltano project that uses a CSV extractor (tap-csv) and `target-bigquery` as the loader. The idea: Meltano reads CSVs from `data/raw/` and writes them to BigQuery tables in a `raw_` dataset.\n",
    "\n",
    "Below are commands and configuration snippets. The notebook will write a minimal `meltano.yml` and show how to run `meltano elt` for each CSV. In production, prefer a single pipeline or orchestrator (Airflow, Cloud Composer, Cloud Workflows) to run Meltano jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8613b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run in terminal \n",
    "gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6412b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote meltano.yml with z3-target-bigquery\n",
      "\n",
      "Next steps (run in a shell or notebook cells):\n",
      "  1) Initialize Meltano project if needed: meltano init olist_project --no-input\n",
      "  2) Register plugins: meltano add extractor tap-csv && meltano add loader target-bigquery\n",
      "  3) Verify plugins: meltano plugins list\n",
      "  4) Run an ELT for a single CSV (example):\n",
      "     meltano elt tap-csv target-bigquery --job_id=olist_customers \\\n",
      "       --config tap-csv:input_path=data/raw/customers.csv \\\n",
      "       --config target-bigquery:dataset=olist_raw\n"
     ]
    }
   ],
   "source": [
    "# Install Meltano and the published BigQuery target package\n",
    "!pip install --quiet meltano z3-target-bigquery\n",
    "\n",
    "import os, textwrap\n",
    "\n",
    "meltano_yml = textwrap.dedent(f\"\"\"\n",
    "version: 1\n",
    "plugins:\n",
    "  extractors:\n",
    "    - name: tap-csv\n",
    "      pip_url: tap-csv\n",
    "  loaders:\n",
    "    - name: target-bigquery\n",
    "      pip_url: z3-target-bigquery\n",
    "  orchestrators: []\n",
    "  utilities: []\n",
    "environments:\n",
    "  local:\n",
    "    variables:\n",
    "      GOOGLE_APPLICATION_CREDENTIALS: \"{os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')}\"\n",
    "      GCP_PROJECT: \"{os.environ.get('GCP_PROJECT')}\"\n",
    "\"\"\")\n",
    "\n",
    "with open('meltano.yml', 'w') as f:\n",
    "    f.write(meltano_yml)\n",
    "\n",
    "print('Wrote meltano.yml with z3-target-bigquery')\n",
    "print()\n",
    "print('Next steps (run in a shell or notebook cells):')\n",
    "print('  1) Initialize Meltano project if needed: meltano init olist_project --no-input')\n",
    "print('  2) Register plugins: meltano add extractor tap-csv && meltano add loader target-bigquery')\n",
    "print('  3) Verify plugins: meltano plugins list')\n",
    "print('  4) Run an ELT for a single CSV (example):')\n",
    "print('     meltano elt tap-csv target-bigquery --job_id=olist_customers \\\\')\n",
    "print('       --config tap-csv:input_path=data/raw/customers.csv \\\\')\n",
    "print('       --config target-bigquery:dataset=olist_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061cbb1",
   "metadata": {},
   "source": [
    "### Example mapping (CSV → BigQuery table)\n",
    "\n",
    "- `olist_customers_dataset.csv` → `olist_raw.raw_customers`\n",
    "- `olist_orders_dataset.csv` → `olist_raw.raw_orders`\n",
    "- `olist_order_items_dataset.csv` → `olist_raw.raw_order_items`\n",
    "- `olist_products_dataset.csv` → `olist_raw.raw_products`\n",
    "- `olist_sellers_dataset.csv` → `olist_raw.raw_sellers`\n",
    "- `olist_order_payments_dataset.csv` → `olist_raw.raw_payments`\n",
    "- `olist_order_reviews_dataset.csv` → `olist_raw.raw_order_reviews`\n",
    "- `olist_geolocation_dataset.csv` → `olist_raw.raw_geolocation`\n",
    "\n",
    "Below is a sample `target-bigquery` config snippet you can set via `meltano config` or in the Meltano UI. It uses the service account JSON via `GOOGLE_APPLICATION_CREDENTIALS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c7dbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample target-bigquery config (for reference):\n",
      "{\n",
      "  \"project_id\": \"algebraic-road-478012-u9\",\n",
      "  \"dataset\": \"olist_raw\",\n",
      "  \"keyfile\": \"/home/dsai/5m-data-2.6-data-pipelines-orchestration/algebraic-road-478012-u9-fe73ba0332c3.json\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "target_bigquery_config = {\n",
    "    'project_id': os.environ['GCP_PROJECT'],\n",
    "    'dataset': 'olist_raw',\n",
    "    # keyfile is optional if GOOGLE_APPLICATION_CREDENTIALS is set\n",
    "    'keyfile': os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n",
    "}\n",
    "import json\n",
    "print('Sample target-bigquery config (for reference):')\n",
    "print(json.dumps(target_bigquery_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85a6f3",
   "metadata": {},
   "source": [
    "## Step 2b: Alternative — direct Python BigQuery upload (useful for quick testing)\n",
    "\n",
    "For quick load CSVs into BigQuery without Meltano for validation, use the Python BigQuery client. This is **not** the Meltano path but is useful for quick checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2806fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset olist_raw exists.\n",
      "\n",
      "Loading /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/customers.csv -> algebraic-road-478012-u9.olist_raw.raw_customers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsai/miniconda3/envs/elt/lib/python3.10/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/customers.csv -> algebraic-road-478012-u9.olist_raw.raw_customers (99441 rows)\n",
      "\n",
      "Loading /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/geolocation.csv -> algebraic-road-478012-u9.olist_raw.raw_geolocation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsai/miniconda3/envs/elt/lib/python3.10/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/geolocation.csv -> algebraic-road-478012-u9.olist_raw.raw_geolocation (1000163 rows)\n",
      "\n",
      "Loading /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/order_items.csv -> algebraic-road-478012-u9.olist_raw.raw_order_items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsai/miniconda3/envs/elt/lib/python3.10/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/order_items.csv -> algebraic-road-478012-u9.olist_raw.raw_order_items (112650 rows)\n",
      "\n",
      "Loading /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/order_payments.csv -> algebraic-road-478012-u9.olist_raw.raw_payments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsai/miniconda3/envs/elt/lib/python3.10/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/order_payments.csv -> algebraic-road-478012-u9.olist_raw.raw_payments (103886 rows)\n",
      "\n",
      "Loading /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/order_reviews.csv -> algebraic-road-478012-u9.olist_raw.raw_order_reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsai/miniconda3/envs/elt/lib/python3.10/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/order_reviews.csv -> algebraic-road-478012-u9.olist_raw.raw_order_reviews (99224 rows)\n",
      "\n",
      "Loading /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/orders.csv -> algebraic-road-478012-u9.olist_raw.raw_orders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsai/miniconda3/envs/elt/lib/python3.10/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/orders.csv -> algebraic-road-478012-u9.olist_raw.raw_orders (99441 rows)\n",
      "\n",
      "Loading /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/products.csv -> algebraic-road-478012-u9.olist_raw.raw_products\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsai/miniconda3/envs/elt/lib/python3.10/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/products.csv -> algebraic-road-478012-u9.olist_raw.raw_products (32951 rows)\n",
      "\n",
      "Loading /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/sellers.csv -> algebraic-road-478012-u9.olist_raw.raw_sellers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsai/miniconda3/envs/elt/lib/python3.10/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/sellers.csv -> algebraic-road-478012-u9.olist_raw.raw_sellers (3095 rows)\n",
      "\n",
      "Loading /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/product_category_name_translation.csv -> algebraic-road-478012-u9.olist_raw.raw_product_category_name_translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsai/miniconda3/envs/elt/lib/python3.10/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/dsai/5m-data-2.6-data-pipelines-orchestration/data/raw/product_category_name_translation.csv -> algebraic-road-478012-u9.olist_raw.raw_product_category_name_translation (71 rows)\n",
      "\n",
      "Loaded tables in dataset olist_raw:\n",
      "- raw_customers\n",
      "- raw_geolocation\n",
      "- raw_order_items\n",
      "- raw_order_reviews\n",
      "- raw_orders\n",
      "- raw_payments\n",
      "- raw_product_category_name_translation\n",
      "- raw_products\n",
      "- raw_sellers\n"
     ]
    }
   ],
   "source": [
    "# Robust CSV -> BigQuery loader\n",
    "!pip install --quiet google-cloud-bigquery pandas pyarrow\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "\n",
    "# Ensure these are set in your environment\n",
    "GCP_PROJECT = os.environ.get('GCP_PROJECT')\n",
    "GCP_CRED = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "RAW_DIR = os.path.join(os.getcwd(), 'data', 'raw')  # adjust if different\n",
    "\n",
    "if not GCP_PROJECT or not GCP_CRED:\n",
    "    raise RuntimeError(\"Set GCP_PROJECT and GOOGLE_APPLICATION_CREDENTIALS environment variables before running.\")\n",
    "\n",
    "client = bigquery.Client(project=GCP_PROJECT)\n",
    "\n",
    "def ensure_dataset(dataset_id: str):\n",
    "    dataset_ref = bigquery.Dataset(f\"{GCP_PROJECT}.{dataset_id}\")\n",
    "    try:\n",
    "        client.get_dataset(dataset_ref)\n",
    "        print(f\"Dataset {dataset_id} exists.\")\n",
    "    except Exception:\n",
    "        print(f\"Dataset {dataset_id} not found — creating it.\")\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = \"US\"  # change region if needed\n",
    "        client.create_dataset(dataset, exists_ok=True)\n",
    "        print(f\"Created dataset {dataset_id}.\")\n",
    "\n",
    "def load_csv_to_bq(local_csv_path, table_name, dataset='olist_raw', chunk_size=200_000):\n",
    "    table_id = f\"{GCP_PROJECT}.{dataset}.{table_name}\"\n",
    "    print(f\"\\nLoading {local_csv_path} -> {table_id}\")\n",
    "    # Try to read a small sample to detect dtypes and parse dates\n",
    "    try:\n",
    "        sample = pd.read_csv(local_csv_path, nrows=1000, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to read CSV sample:\", e)\n",
    "        raise\n",
    "\n",
    "    # Heuristic: parse common timestamp columns if present\n",
    "    date_cols = [c for c in sample.columns if 'date' in c.lower() or 'timestamp' in c.lower()]\n",
    "    parse_dates = date_cols if date_cols else None\n",
    "\n",
    "    # Try full read with parse_dates if any\n",
    "    try:\n",
    "        df = pd.read_csv(local_csv_path, parse_dates=parse_dates, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(\"Full read failed, attempting chunked read:\", e)\n",
    "        # Fallback: load in chunks and append to BigQuery\n",
    "        try:\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "            # First chunk: create/replace table\n",
    "            first = True\n",
    "            for chunk in pd.read_csv(local_csv_path, chunksize=chunk_size, parse_dates=parse_dates, low_memory=False):\n",
    "                if first:\n",
    "                    job = client.load_table_from_dataframe(chunk, table_id, job_config=job_config)\n",
    "                    job.result()\n",
    "                    first = False\n",
    "                else:\n",
    "                    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "                    job = client.load_table_from_dataframe(chunk, table_id, job_config=job_config)\n",
    "                    job.result()\n",
    "            print(f\"Chunked load complete for {local_csv_path}\")\n",
    "            return\n",
    "        except GoogleAPIError as ge:\n",
    "            print(\"BigQuery chunked load failed:\", ge)\n",
    "            raise\n",
    "\n",
    "    # If we have a dataframe, attempt a single load\n",
    "    try:\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "        )\n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        job.result()\n",
    "        print(f\"Loaded {local_csv_path} -> {table_id} ({df.shape[0]} rows)\")\n",
    "    except GoogleAPIError as ge:\n",
    "        print(\"BigQuery load failed:\", ge)\n",
    "        # As a fallback, try chunked upload\n",
    "        try:\n",
    "            print(\"Retrying with chunked upload...\")\n",
    "            first = True\n",
    "            for chunk in pd.read_csv(local_csv_path, chunksize=chunk_size, parse_dates=parse_dates, low_memory=False):\n",
    "                if first:\n",
    "                    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "                    job = client.load_table_from_dataframe(chunk, table_id, job_config=job_config)\n",
    "                    job.result()\n",
    "                    first = False\n",
    "                else:\n",
    "                    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "                    job = client.load_table_from_dataframe(chunk, table_id, job_config=job_config)\n",
    "                    job.result()\n",
    "            print(\"Chunked retry succeeded.\")\n",
    "        except Exception as e:\n",
    "            print(\"Chunked retry also failed:\", e)\n",
    "            raise\n",
    "\n",
    "# Ensure dataset exists\n",
    "ensure_dataset('olist_raw')\n",
    "\n",
    "# Files to load (adjust list/order as you like)\n",
    "files_and_tables = {\n",
    "    'olist_customers_dataset.csv': 'raw_customers',\n",
    "    'olist_geolocation_dataset.csv': 'raw_geolocation',\n",
    "    'olist_order_items_dataset.csv': 'raw_order_items',\n",
    "    'olist_order_payments_dataset.csv': 'raw_payments',\n",
    "    'olist_order_reviews_dataset.csv': 'raw_order_reviews',\n",
    "    'olist_orders_dataset.csv': 'raw_orders',\n",
    "    'olist_products_dataset.csv': 'raw_products',\n",
    "    'olist_sellers_dataset.csv': 'raw_sellers',\n",
    "    'product_category_name_translation.csv': 'raw_product_category_name_translation'\n",
    "}\n",
    "\n",
    "for fname, table in files_and_tables.items():\n",
    "    path = os.path.join(RAW_DIR, fname)\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            load_csv_to_bq(path, table)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {fname}: {e}\")\n",
    "    else:\n",
    "        print(f\"{fname} not found in {RAW_DIR}\")\n",
    "\n",
    "# List loaded tables\n",
    "print(\"\\nLoaded tables in dataset olist_raw:\")\n",
    "try:\n",
    "    tables = client.list_tables('olist_raw')\n",
    "    for table in tables:\n",
    "        print('-', table.table_id)\n",
    "except Exception as e:\n",
    "    print(\"Failed to list tables:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
